{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5SRakhq3zXpj"
      },
      "outputs": [],
      "source": [
        "# Install all required packages\n",
        "!pip install pypdf2 pdf2image pdfplumber pytesseract opencv-python-headless pillow sentence-transformers faiss-cpu nltk\n",
        "\n",
        "# Import all necessary libraries\n",
        "import os\n",
        "import io\n",
        "import fitz\n",
        "import pdfplumber\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import cv2\n",
        "from google.colab import files\n",
        "import faiss\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from typing import List, Dict, Any\n",
        "import json\n",
        "\n",
        "class DocumentChunker:\n",
        "    def __init__(self, chunk_size: int = 500, chunk_overlap: int = 50):\n",
        "        self.chunk_size = chunk_size\n",
        "        self.chunk_overlap = chunk_overlap\n",
        "\n",
        "    def create_chunks(self, text: str) -> List[str]:\n",
        "        \"\"\"\n",
        "        Split text into overlapping chunks using a simple character-based approach\n",
        "        \"\"\"\n",
        "        if not text:\n",
        "            return []\n",
        "\n",
        "        # Split text into words\n",
        "        words = text.split()\n",
        "\n",
        "        if not words:\n",
        "            return []\n",
        "\n",
        "        chunks = []\n",
        "        current_chunk = []\n",
        "        current_length = 0\n",
        "\n",
        "        for word in words:\n",
        "            current_length += len(word) + 1  # +1 for space\n",
        "            current_chunk.append(word)\n",
        "\n",
        "            if current_length >= self.chunk_size:\n",
        "                # Add chunk to list\n",
        "                chunks.append(' '.join(current_chunk))\n",
        "\n",
        "                # Keep overlap words for next chunk\n",
        "                overlap_words = current_chunk[-self.chunk_overlap:]\n",
        "                current_chunk = overlap_words\n",
        "                current_length = sum(len(word) + 1 for word in overlap_words)\n",
        "\n",
        "        # Add the last chunk if it exists and is not too small\n",
        "        if current_chunk and current_length > self.chunk_overlap:\n",
        "            chunks.append(' '.join(current_chunk))\n",
        "\n",
        "        return chunks\n",
        "\n",
        "class VectorStore:\n",
        "    def __init__(self, model_name: str = 'all-MiniLM-L6-v2'):\n",
        "        self.encoder = SentenceTransformer(model_name)\n",
        "        self.index = None\n",
        "        self.chunks = []\n",
        "        self.metadata = []\n",
        "\n",
        "    def add_documents(self, chunks: List[str], metadata: List[Dict[str, Any]] = None):\n",
        "        if not chunks:\n",
        "            return\n",
        "\n",
        "        embeddings = self.encoder.encode(chunks)\n",
        "\n",
        "        if self.index is None:\n",
        "            dimension = embeddings.shape[1]\n",
        "            self.index = faiss.IndexFlatL2(dimension)\n",
        "\n",
        "        self.index.add(np.array(embeddings).astype('float32'))\n",
        "\n",
        "        start_idx = len(self.chunks)\n",
        "        self.chunks.extend(chunks)\n",
        "\n",
        "        if metadata is None:\n",
        "            metadata = [{\"index\": i} for i in range(start_idx, start_idx + len(chunks))]\n",
        "        self.metadata.extend(metadata)\n",
        "\n",
        "    def search(self, query: str, k: int = 5) -> List[Dict[str, Any]]:\n",
        "        query_embedding = self.encoder.encode([query])\n",
        "        distances, indices = self.index.search(np.array(query_embedding).astype('float32'), k)\n",
        "\n",
        "        results = []\n",
        "        for idx, distance in zip(indices[0], distances[0]):\n",
        "            if idx < len(self.chunks):\n",
        "                results.append({\n",
        "                    'chunk': self.chunks[idx],\n",
        "                    'metadata': self.metadata[idx],\n",
        "                    'distance': float(distance)\n",
        "                })\n",
        "\n",
        "        return results\n",
        "\n",
        "    def save(self, path: str):\n",
        "        if self.index is not None:\n",
        "            faiss.write_index(self.index, f\"{path}_index.faiss\")\n",
        "            with open(f\"{path}_data.json\", 'w', encoding='utf-8') as f:\n",
        "                json.dump({\n",
        "                    'chunks': self.chunks,\n",
        "                    'metadata': self.metadata\n",
        "                }, f, ensure_ascii=False)\n",
        "\n",
        "    @classmethod\n",
        "    def load(cls, path: str):\n",
        "        instance = cls()\n",
        "        instance.index = faiss.read_index(f\"{path}_index.faiss\")\n",
        "        with open(f\"{path}_data.json\", 'r', encoding='utf-8') as f:\n",
        "            data = json.load(f)\n",
        "            instance.chunks = data['chunks']\n",
        "            instance.metadata = data['metadata']\n",
        "        return instance\n",
        "\n",
        "def extract_all_from_pdf(pdf_path):\n",
        "    \"\"\"\n",
        "    Extract text, tables, and images from a PDF file\n",
        "    \"\"\"\n",
        "    extracted_data = {\n",
        "        'text': [],\n",
        "        'tables': [],\n",
        "        'images': []\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        with pdfplumber.open(pdf_path) as pdf:\n",
        "            for page_num, page in enumerate(pdf.pages):\n",
        "                # Extract text\n",
        "                text = page.extract_text()\n",
        "                if text:\n",
        "                    extracted_data['text'].append({\n",
        "                        'page': page_num + 1,\n",
        "                        'content': text\n",
        "                    })\n",
        "\n",
        "                # Extract tables\n",
        "                tables = page.extract_tables()\n",
        "                if tables:\n",
        "                    extracted_data['tables'].extend([{\n",
        "                        'page': page_num + 1,\n",
        "                        'content': table\n",
        "                    } for table in tables])\n",
        "\n",
        "        # Image Extraction\n",
        "        doc = fitz.open(pdf_path)\n",
        "        for page_num, page in enumerate(doc):\n",
        "            image_list = page.get_images()\n",
        "\n",
        "            for img_index, img in enumerate(image_list):\n",
        "                xref = img[0]\n",
        "                base_image = doc.extract_image(xref)\n",
        "                image_bytes = base_image[\"image\"]\n",
        "\n",
        "                image = Image.open(io.BytesIO(image_bytes))\n",
        "\n",
        "                extracted_data['images'].append({\n",
        "                    'page': page_num + 1,\n",
        "                    'index': img_index,\n",
        "                    'image': image\n",
        "                })\n",
        "\n",
        "        doc.close()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during extraction: {str(e)}\")\n",
        "\n",
        "    return extracted_data\n",
        "\n",
        "def save_extracted_data(extracted_data, output_dir):\n",
        "    \"\"\"\n",
        "    Save extracted content to files\n",
        "    \"\"\"\n",
        "    try:\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "        # Save text\n",
        "        for i, text_item in enumerate(extracted_data['text']):\n",
        "            with open(f\"{output_dir}/page_{text_item['page']}_text.txt\", 'w', encoding='utf-8') as f:\n",
        "                f.write(text_item['content'])\n",
        "\n",
        "        # Save tables as CSV\n",
        "        import csv\n",
        "        for i, table_item in enumerate(extracted_data['tables']):\n",
        "            with open(f\"{output_dir}/page_{table_item['page']}_table_{i}.csv\", 'w', newline='', encoding='utf-8') as f:\n",
        "                writer = csv.writer(f)\n",
        "                writer.writerows(table_item['content'])\n",
        "\n",
        "        # Save images\n",
        "        for i, img_item in enumerate(extracted_data['images']):\n",
        "            img_path = f\"{output_dir}/page_{img_item['page']}_image_{img_item['index']}.png\"\n",
        "            img_item['image'].save(img_path)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving data: {str(e)}\")\n",
        "\n",
        "def process_pdf_complete(pdf_path: str, output_dir: str, chunk_size: int = 500):\n",
        "    \"\"\"\n",
        "    Complete pipeline: Extract content, create chunks, and build vector store\n",
        "    \"\"\"\n",
        "    print(f\"Processing PDF: {pdf_path}\")\n",
        "\n",
        "    # Step 1: Extract all content\n",
        "    print(\"Extracting content...\")\n",
        "    extracted_data = extract_all_from_pdf(pdf_path)\n",
        "\n",
        "    # Step 2: Initialize chunker and vector store\n",
        "    print(\"Initializing chunking and vector storage...\")\n",
        "    chunker = DocumentChunker(chunk_size=chunk_size)\n",
        "    vector_store = VectorStore()\n",
        "\n",
        "    # Step 3: Process and chunk content\n",
        "    print(\"Processing text and creating chunks...\")\n",
        "    all_chunks = []\n",
        "    all_metadata = []\n",
        "\n",
        "    # Process text\n",
        "    for text_item in extracted_data['text']:\n",
        "        chunks = chunker.create_chunks(text_item['content'])\n",
        "        all_chunks.extend(chunks)\n",
        "        chunk_metadata = [{\n",
        "            'type': 'text',\n",
        "            'page': text_item['page'],\n",
        "            'chunk_index': i\n",
        "        } for i in range(len(chunks))]\n",
        "        all_metadata.extend(chunk_metadata)\n",
        "\n",
        "    # Process tables\n",
        "    for table_item in extracted_data['tables']:\n",
        "        table_text = '\\n'.join([' '.join(map(str, row)) for row in table_item['content']])\n",
        "        chunks = chunker.create_chunks(table_text)\n",
        "        all_chunks.extend(chunks)\n",
        "        chunk_metadata = [{\n",
        "            'type': 'table',\n",
        "            'page': table_item['page'],\n",
        "            'chunk_index': i\n",
        "        } for i in range(len(chunks))]\n",
        "        all_metadata.extend(chunk_metadata)\n",
        "\n",
        "    # Step 4: Add to vector store\n",
        "    print(\"Creating embeddings and building vector index...\")\n",
        "    if all_chunks:\n",
        "        vector_store.add_documents(all_chunks, all_metadata)\n",
        "\n",
        "    # Step 5: Save everything\n",
        "    print(\"Saving extracted content and vector store...\")\n",
        "    save_extracted_data(extracted_data, output_dir)\n",
        "    vector_store.save(f\"{output_dir}/vector_store\")\n",
        "\n",
        "    print(f\"\\nExtraction Summary:\")\n",
        "    print(f\"Pages with text: {len(extracted_data['text'])}\")\n",
        "    print(f\"Tables found: {len(extracted_data['tables'])}\")\n",
        "    print(f\"Images found: {len(extracted_data['images'])}\")\n",
        "    print(f\"Total chunks created: {len(all_chunks)}\")\n",
        "    print(f\"\\nAll content saved to: {output_dir}\")\n",
        "\n",
        "    return vector_store\n",
        "\n",
        "def main():\n",
        "    print(\"Upload a PDF file:\")\n",
        "    uploaded = files.upload()\n",
        "\n",
        "    for filename in uploaded.keys():\n",
        "        output_dir = f'extracted_{os.path.splitext(filename)[0]}'\n",
        "        try:\n",
        "            vector_store = process_pdf_complete(filename, output_dir)\n",
        "\n",
        "            # Test search\n",
        "            print(\"\\nTesting vector search...\")\n",
        "            query = \"What is the main topic?\"\n",
        "            results = vector_store.search(query, k=3)\n",
        "\n",
        "            print(f\"\\nTop 3 results for query: '{query}'\")\n",
        "            for i, result in enumerate(results):\n",
        "                print(f\"\\nResult {i+1}:\")\n",
        "                print(f\"Content: {result['chunk'][:200]}...\")\n",
        "                print(f\"Page: {result['metadata']['page']}\")\n",
        "                print(f\"Type: {result['metadata']['type']}\")\n",
        "                print(f\"Distance: {result['distance']:.4f}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {filename}: {str(e)}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "from typing import List, Dict, Any, Optional\n",
        "from dataclasses import dataclass\n",
        "import json\n",
        "import pandas as pd\n",
        "import os\n",
        "import glob\n",
        "\n",
        "@dataclass\n",
        "class SearchResult:\n",
        "    chunk: str\n",
        "    metadata: Dict[str, Any]\n",
        "    similarity_score: float\n",
        "    table_data: Optional[List[List[str]]] = None\n",
        "\n",
        "def read_csv_table(output_dir: str, page_number: int, table_index: int = 0) -> Optional[List[List[str]]]:\n",
        "    \"\"\"\n",
        "    Read a specific table from the saved CSV files\n",
        "    \"\"\"\n",
        "    try:\n",
        "        csv_path = os.path.join(output_dir, f\"page_{page_number}_table_{table_index}.csv\")\n",
        "        if os.path.exists(csv_path):\n",
        "            df = pd.read_csv(csv_path)\n",
        "            # Convert DataFrame to list of lists, including column headers\n",
        "            table_data = [df.columns.tolist()] + df.values.tolist()\n",
        "            return table_data\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading CSV file: {e}\")\n",
        "        return None\n",
        "\n",
        "def get_tables_for_page(output_dir: str, page_number: int) -> List[List[List[str]]]:\n",
        "    \"\"\"\n",
        "    Get all tables from a specific page\n",
        "    \"\"\"\n",
        "    tables = []\n",
        "    pattern = os.path.join(output_dir, f\"page_{page_number}_table_*.csv\")\n",
        "    for csv_file in sorted(glob.glob(pattern)):\n",
        "        try:\n",
        "            df = pd.read_csv(csv_file)\n",
        "            table_data = [df.columns.tolist()] + df.values.tolist()\n",
        "            tables.append(table_data)\n",
        "        except Exception as e:\n",
        "            print(f\"Error reading {csv_file}: {e}\")\n",
        "    return tables\n",
        "\n",
        "def format_table(table_data: List[List[Any]]) -> str:\n",
        "    \"\"\"Format table data into a readable string\"\"\"\n",
        "    if not table_data:\n",
        "        return \"\"\n",
        "\n",
        "    # Convert all values to strings and handle None/empty values\n",
        "    table_data = [[str(cell) if cell is not None else '' for cell in row] for row in table_data]\n",
        "\n",
        "    # Calculate column widths\n",
        "    col_widths = [max(len(str(cell)) for cell in col) for col in zip(*table_data)]\n",
        "\n",
        "    # Create formatted rows\n",
        "    formatted_rows = []\n",
        "    for row in table_data:\n",
        "        formatted_row = \" | \".join(f\"{str(cell):<{width}}\" for cell, width in zip(row, col_widths))\n",
        "        formatted_rows.append(f\"| {formatted_row} |\")\n",
        "\n",
        "    # Create separator\n",
        "    separator = \"+{}+\".format(\"+\".join(\"-\" * (width + 2) for width in col_widths))\n",
        "\n",
        "    # Combine everything\n",
        "    return \"\\n\".join([\n",
        "        separator,\n",
        "        formatted_rows[0],\n",
        "        separator,\n",
        "        *formatted_rows[1:],\n",
        "        separator\n",
        "    ])\n",
        "\n",
        "class QueryHandler:\n",
        "    # def __init__(self, vector_store, output_dir: str):\n",
        "    #     \"\"\"\n",
        "    #     Initialize QueryHandler with a vector store and output directory.\n",
        "\n",
        "    #     Args:\n",
        "    #         vector_store: Vector store containing document embeddings\n",
        "    #         output_dir: Directory containing extracted document content\n",
        "    #     \"\"\"\n",
        "    #     self.vector_store = vector_store\n",
        "    #     self.output_dir = output_dir\n",
        "    #     self.image_cache = {}  # Cache for analyzed images\n",
        "\n",
        "    #     # Initialize the cosine similarity index if vector store has an index\n",
        "    #     if hasattr(vector_store, 'index') and vector_store.index is not None:\n",
        "    #         dimension = vector_store.index.d\n",
        "    #         self._replace_index_with_cosine(dimension)\n",
        "\n",
        "    # def _replace_index_with_cosine(self, dimension: int):\n",
        "    #     \"\"\"\n",
        "    #     Replace the existing index with a cosine similarity index.\n",
        "    #     \"\"\"\n",
        "    #     new_index = faiss.IndexFlatIP(dimension)\n",
        "\n",
        "    #     if self.vector_store.index.ntotal > 0:\n",
        "    #         vectors = faiss.vector_to_array(self.vector_store.index.reconstruct_n(0, self.vector_store.index.ntotal))\n",
        "    #         vectors = vectors.reshape(-1, dimension)\n",
        "    #         faiss.normalize_L2(vectors)\n",
        "    #         new_index.add(vectors)\n",
        "\n",
        "    #     self.vector_store.index = new_index\n",
        "\n",
        "    # def get_table_data(self, page_number: int) -> List[Dict[str, Any]]:\n",
        "    #     \"\"\"\n",
        "    #     Get table data for a specific page.\n",
        "    #     \"\"\"\n",
        "    #     # Implement table extraction logic here\n",
        "    #     # This should return a list of dictionaries containing table data\n",
        "    #     return []\n",
        "\n",
        "    # def get_image_data(self, page_number: Optional[int] = None) -> List[ImageData]:\n",
        "    #     \"\"\"\n",
        "    #     Get and analyze images for a specific page.\n",
        "    #     \"\"\"\n",
        "    #     image_data = []\n",
        "    #     if page_number is not None:\n",
        "    #         image_paths = get_images_for_page(self.output_dir, page_number)\n",
        "    #         for img_path in image_paths:\n",
        "    #             if img_path not in self.image_cache:\n",
        "    #                 self.image_cache[img_path] = analyze_image(img_path)\n",
        "    #             if self.image_cache[img_path]:\n",
        "    #                 image_data.append(self.image_cache[img_path])\n",
        "    #     return image_data\n",
        "\n",
        "    # def search(self, query: str, page_number: Optional[int] = None, k: int = 5) -> List[SearchResult]:\n",
        "    #     \"\"\"\n",
        "    #     Search for relevant content based on the query.\n",
        "    #     \"\"\"\n",
        "    #     results = []\n",
        "    #     query_lower = query.lower()\n",
        "\n",
        "    #     # Handle image-specific queries\n",
        "    #     if any(term in query_lower for term in ['image', 'chart', 'graph', 'unemployment', 'figure']):\n",
        "    #         if page_number is not None:\n",
        "    #             image_data = self.get_image_data(page_number)\n",
        "    #             for img_data in image_data:\n",
        "    #                 results.append(SearchResult(\n",
        "    #                     chunk=img_data.content,\n",
        "    #                     metadata={\n",
        "    #                         'type': 'image',\n",
        "    #                         'page': page_number,\n",
        "    #                         'path': img_data.image_path\n",
        "    #                     },\n",
        "    #                     similarity_score=1.0,\n",
        "    #                     image_data=img_data\n",
        "    #                 ))\n",
        "\n",
        "    #     # Handle table queries\n",
        "    #     if 'table' in query_lower or 'tabular' in query_lower:\n",
        "    #         table_data = self.get_table_data(page_number) if page_number is not None else []\n",
        "    #         for table_info in table_data:\n",
        "    #             results.append(SearchResult(\n",
        "    #                 chunk=f\"Table from page {table_info['page']}\",\n",
        "    #                 metadata={\n",
        "    #                     'type': 'table',\n",
        "    #                     'page': table_info['page'],\n",
        "    #                     'table_index': table_info['table_index']\n",
        "    #                 },\n",
        "    #                 similarity_score=1.0,\n",
        "    #                 table_data=table_info['table_data']\n",
        "    #             ))\n",
        "\n",
        "    #     # Perform vector search if no specific results found\n",
        "    #     if not results and hasattr(self.vector_store, 'search'):\n",
        "    #         vector_results = self.vector_store.search(query, k=k)\n",
        "    #         if vector_results:\n",
        "    #             for result in vector_results:\n",
        "    #                 if page_number is None or result.metadata['page'] == page_number:\n",
        "    #                     results.append(result)\n",
        "\n",
        "    #     return results[:k]\n",
        "    # ... rest of the QueryHandler class methods remain the same ...\n",
        "    def __init__(self, vector_store: VectorStore, output_dir: str):\n",
        "        self.vector_store = vector_store\n",
        "        self.output_dir = output_dir\n",
        "        if vector_store.index is not None:\n",
        "            dimension = vector_store.index.d\n",
        "            self._replace_index_with_cosine(dimension)\n",
        "\n",
        "    def _replace_index_with_cosine(self, dimension: int):\n",
        "        self.vector_store.index = faiss.IndexFlatIP(dimension)\n",
        "        if self.vector_store.chunks:\n",
        "            embeddings = self.vector_store.encoder.encode(self.vector_store.chunks)\n",
        "            faiss.normalize_L2(embeddings)\n",
        "            self.vector_store.index.add(embeddings)\n",
        "\n",
        "    def get_table_data(self, page_number: Optional[int] = None) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Get table data for a specific page or all pages\n",
        "        \"\"\"\n",
        "        if page_number is not None:\n",
        "            tables = get_tables_for_page(self.output_dir, page_number)\n",
        "            return [{\n",
        "                'page': page_number,\n",
        "                'table_index': idx,\n",
        "                'table_data': table\n",
        "            } for idx, table in enumerate(tables)]\n",
        "        return []\n",
        "\n",
        "    def search(self, query: str, page_number: Optional[int] = None, k: int = 5) -> List[SearchResult]:\n",
        "        results = []\n",
        "\n",
        "        # If query is about tables, prioritize table retrieval\n",
        "        query_lower = query.lower()\n",
        "        if 'table' in query_lower or 'tabular' in query_lower:\n",
        "            if page_number is not None:\n",
        "                table_data = self.get_table_data(page_number)\n",
        "                for table_info in table_data:\n",
        "                    results.append(SearchResult(\n",
        "                        chunk=f\"Table from page {table_info['page']}\",\n",
        "                        metadata={\n",
        "                            'type': 'table',\n",
        "                            'page': table_info['page'],\n",
        "                            'table_index': table_info['table_index']\n",
        "                        },\n",
        "                        similarity_score=1.0,  # Direct match for tables\n",
        "                        table_data=table_info['table_data']\n",
        "                    ))\n",
        "\n",
        "        # If no tables found or query is not about tables, perform regular search\n",
        "        if not results:\n",
        "            query_embedding = self.vector_store.encoder.encode([query])\n",
        "            faiss.normalize_L2(query_embedding)\n",
        "\n",
        "            similarities, indices = self.vector_store.index.search(\n",
        "                query_embedding,\n",
        "                min(k, len(self.vector_store.chunks))\n",
        "            )\n",
        "\n",
        "            for idx, similarity in zip(indices[0], similarities[0]):\n",
        "                if idx < len(self.vector_store.chunks):\n",
        "                    metadata = self.vector_store.metadata[idx]\n",
        "                    if page_number is None or metadata.get('page') == page_number:\n",
        "                        result = SearchResult(\n",
        "                            chunk=self.vector_store.chunks[idx],\n",
        "                            metadata=metadata,\n",
        "                            similarity_score=float(similarity)\n",
        "                        )\n",
        "\n",
        "                        # Try to load table data if it's a table type\n",
        "                        if metadata.get('type') == 'table':\n",
        "                            table_data = read_csv_table(\n",
        "                                self.output_dir,\n",
        "                                metadata['page'],\n",
        "                                metadata.get('table_index', 0)\n",
        "                            )\n",
        "                            if table_data:\n",
        "                                result.table_data = table_data\n",
        "\n",
        "                        results.append(result)\n",
        "\n",
        "        return results[:k]\n",
        "\n",
        "def process_query(vector_store: VectorStore, query: str, output_dir: str, k: int = 5) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Process a user query and return relevant results with metadata\n",
        "    \"\"\"\n",
        "    query_handler = QueryHandler(vector_store, output_dir)\n",
        "\n",
        "    # Extract page number if mentioned in query\n",
        "    page_number = None\n",
        "    query_lower = query.lower()\n",
        "    if \"page\" in query_lower:\n",
        "        try:\n",
        "            words = query_lower.split()\n",
        "            page_idx = words.index(\"page\")\n",
        "            if page_idx + 1 < len(words):\n",
        "                page_number = int(words[page_idx + 1])\n",
        "        except (ValueError, IndexError):\n",
        "            pass\n",
        "\n",
        "    # Perform search\n",
        "    results = query_handler.search(query, page_number=page_number, k=k)\n",
        "\n",
        "    # Generate response\n",
        "    if not results:\n",
        "        prompt = f\"Question: {query}\\n\\nI could not find any relevant information in the document to answer this question.\"\n",
        "    else:\n",
        "        context_parts = []\n",
        "        for result in results:\n",
        "            if result.table_data:\n",
        "                context_parts.append(f\"\\nTable from page {result.metadata['page']}:\\n{format_table(result.table_data)}\")\n",
        "            else:\n",
        "                context_parts.append(f\"[Page {result.metadata['page']}] {result.chunk}\")\n",
        "\n",
        "        prompt = f\"\"\"Please answer the following question based on this context from the document:\n",
        "\n",
        "Context:\n",
        "{' '.join(context_parts)}\n",
        "\n",
        "Question: {query}\n",
        "\n",
        "Answer:\"\"\"\n",
        "\n",
        "    return {\n",
        "        'results': results,\n",
        "        'prompt': prompt,\n",
        "        'page_number': page_number\n",
        "    }\n",
        "\n",
        "def main():\n",
        "    print(\"Upload a PDF file:\")\n",
        "    uploaded = files.upload()\n",
        "\n",
        "    for filename in uploaded.keys():\n",
        "        output_dir = f'extracted_{os.path.splitext(filename)[0]}'\n",
        "        try:\n",
        "            vector_store = process_pdf_complete(filename, output_dir)\n",
        "\n",
        "            print(f\"\\nDocument processed successfully!\")\n",
        "            print(f\"Total chunks: {len(vector_store.chunks)}\")\n",
        "            print(\"\\nYou can now ask questions about the document.\")\n",
        "            print(\"To ask about specific pages, include 'page X' in your question.\")\n",
        "            print(\"Type 'quit' to exit.\")\n",
        "\n",
        "            while True:\n",
        "                print(\"\\n\" + \"-\"*80)\n",
        "                query = input(\"Enter your question: \").strip()\n",
        "                if query.lower() == 'quit':\n",
        "                    break\n",
        "\n",
        "                if not query:\n",
        "                    print(\"Please enter a question.\")\n",
        "                    continue\n",
        "\n",
        "                results = process_query(vector_store, query, output_dir)\n",
        "                display_results(results)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {filename}: {str(e)}\")\n",
        "            raise e\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "PJNB5IPpzdai"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question: table from page 6?"
      ],
      "metadata": {
        "id": "ZAH72ynwzklU"
      }
    }
  ]
}